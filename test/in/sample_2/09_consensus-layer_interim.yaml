---
- :key: '1'
  :contents: "`# Consensus layer considerations`"
  :translated_contents: ''
  :original_contents: "`# Consensus layer considerations` "
- :key: '2'
  :contents: HARP is designed so that it can work with different implementations of  consensus
    layer, also known as Distributed Control Systems (DCS).
  :translated_contents: ''
  :original_contents: 'HARP is designed so that it can work with different implementations
    of  consensus layer, also known as Distributed Control Systems (DCS). '
- :key: '3'
  :contents: 'Currently the following DCS implementations are supported:'
  :translated_contents: ''
  :original_contents: 'Currently the following DCS implementations are supported: '
- :key: '4'
  :contents: " - etcd - BDR "
  :translated_contents: ''
  :original_contents: " - etcd - BDR "
- :key: '5'
  :contents: This information is specific to HARP's interaction with the  supported
    DCS implementations.
  :translated_contents: ''
  :original_contents: 'This information is specific to HARP''s interaction with the  supported
    DCS implementations. '
- :key: '6'
  :contents: "## BDR driver compatibility"
  :translated_contents: ''
  :original_contents: "## BDR driver compatibility "
- :key: '7'
  :contents: The `bdr` native consensus layer is available from BDR versions  edb_lk_asis_1   and
    edb_lk_asis_2 .
  :translated_contents: ''
  :original_contents: 'The `bdr` native consensus layer is available from BDR versions  edb_lk_asis_1   and
    edb_lk_asis_2 . '
- :key: '8'
  :contents: For the purpose of maintaining a voting quorum, BDR Logical Standby  nodes
    don't participate in consensus communications in a BDR cluster. Don't count these
    in the total node list to fulfill DCS quorum requirements.
  :translated_contents: ''
  :original_contents: 'For the purpose of maintaining a voting quorum, BDR Logical
    Standby  nodes don''t participate in consensus communications in a BDR cluster.
    Don''t count these in the total node list to fulfill DCS quorum requirements. '
- :key: '9'
  :contents: "## Maintaining quorum"
  :translated_contents: ''
  :original_contents: "## Maintaining quorum "
- :key: '10'
  :contents: Clusters of any architecture require at least n/2 + 1 nodes to maintain  consensus
    via a voting quorum. Thus a three-node cluster can tolerate the outage of  a single
    node, a five-node cluster can tolerate a two-node outage, and so on. If  consensus
    is ever lost, HARP becomes inoperable because the DCS prevents it  from deterministically
    identifying the node that is the lead master in a  particular location.
  :translated_contents: ''
  :original_contents: 'Clusters of any architecture require at least n/2 + 1 nodes
    to maintain  consensus via a voting quorum. Thus a three-node cluster can tolerate
    the outage of  a single node, a five-node cluster can tolerate a two-node outage,
    and so on. If  consensus is ever lost, HARP becomes inoperable because the DCS
    prevents it  from deterministically identifying the node that is the lead master
    in a  particular location. '
- :key: '11'
  :contents: As a result, whichever DCS is chosen, more than half of the nodes must
    always  be available _cluster-wide_. This can become a non-trivial element when  distributing
    DCS nodes among two or more data centers. A network partition  prevents quorum
    in any location that can't maintain a voting majority, and thus  HARP stops working.
  :translated_contents: ''
  :original_contents: 'As a result, whichever DCS is chosen, more than half of the
    nodes must always  be available _cluster-wide_. This can become a non-trivial
    element when  distributing DCS nodes among two or more data centers. A network
    partition  prevents quorum in any location that can''t maintain a voting majority,
    and thus  HARP stops working. '
- :key: '12'
  :contents: Thus an odd-number of nodes (with a minimum of three) is crucial when
    building the  consensus layer. An ideal case distributes nodes across a minimum
    of  three independent locations to prevent a single network partition from  disrupting
    consensus.
  :translated_contents: ''
  :original_contents: 'Thus an odd-number of nodes (with a minimum of three) is crucial
    when building the  consensus layer. An ideal case distributes nodes across a minimum
    of  three independent locations to prevent a single network partition from  disrupting
    consensus. '
- :key: '13'
  :contents: One example configuration is to designate two DCS nodes in two data centers  coinciding
    with the primary BDR nodes, and a fifth DCS node (such as a BDR  witness) elsewhere.
    Using such a design, a network partition between the two  BDR data centers doesn't
    disrupt consensus thanks to the independently located node.
  :translated_contents: ''
  :original_contents: 'One example configuration is to designate two DCS nodes in
    two data centers  coinciding with the primary BDR nodes, and a fifth DCS node
    (such as a BDR  witness) elsewhere. Using such a design, a network partition between
    the two  BDR data centers doesn''t disrupt consensus thanks to the independently
    located node. '
- :key: '14'
  :contents: "### Multi-consensus variant"
  :translated_contents: ''
  :original_contents: "### Multi-consensus variant "
- :key: '15'
  :contents: HARP assumes one lead master per configured location. Normally each  location
    is specified in HARP using the `location` configuration setting.  By creating
    a separate DCS cluster per location, you can emulate this behavior independently
    of HARP.
  :translated_contents: ''
  :original_contents: 'HARP assumes one lead master per configured location. Normally
    each  location is specified in HARP using the `location` configuration setting.  By
    creating a separate DCS cluster per location, you can emulate this behavior independently
    of HARP. '
- :key: '16'
  :contents: To accomplish this, configure HARP in `config.yml` to use a different
    DCS connection target per desired Location.
  :translated_contents: ''
  :original_contents: 'To accomplish this, configure HARP in `config.yml` to use a
    different DCS connection target per desired Location. '
- :key: '17'
  :contents: 'HARP nodes in DC-A use something like this:'
  :translated_contents: ''
  :original_contents: 'HARP nodes in DC-A use something like this: '
- :key: '18'
  :contents: edb_notranlate_0
  :translated_contents: ''
  :original_contents: " edb_notranlate_0  "
- :key: '19'
  :contents: 'While DC-B uses different hostnames corresponding to nodes in its  canonical
    location:'
  :translated_contents: ''
  :original_contents: 'While DC-B uses different hostnames corresponding to nodes
    in its  canonical location: '
- :key: '20'
  :contents: edb_notranlate_1
  :translated_contents: ''
  :original_contents: " edb_notranlate_1  "
- :key: '21'
  :contents: There's no DCS communication between different data centers in this design,  and
    thus a network partition between them doesn't affect HARP operation. A  consequence
    of this is that HARP is completely unaware of nodes in the other  location, and
    each location operates essentially as a separate HARP cluster.
  :translated_contents: ''
  :original_contents: 'There''s no DCS communication between different data centers
    in this design,  and thus a network partition between them doesn''t affect HARP
    operation. A  consequence of this is that HARP is completely unaware of nodes
    in the other  location, and each location operates essentially as a separate HARP
    cluster. '
- :key: '22'
  :contents: This isn't possible when using BDR as the DCS, as BDR maintains a consensus  layer
    across all participant nodes.
  :translated_contents: ''
  :original_contents: 'This isn''t possible when using BDR as the DCS, as BDR maintains
    a consensus  layer across all participant nodes.  '
- :key: '23'
  :contents: A possible drawback to this approach is that `harpctl` can't interact  with
    nodes outside of the current location. It's impossible to obtain  node information,
    get or set the lead master, or perform any other operation that  targets the other
    location. Essentially this organization renders the  `--location` parameter to
    `harpctl` unusable.
  :translated_contents: ''
  :original_contents: 'A possible drawback to this approach is that `harpctl` can''t
    interact  with nodes outside of the current location. It''s impossible to obtain  node
    information, get or set the lead master, or perform any other operation that  targets
    the other location. Essentially this organization renders the  `--location` parameter
    to `harpctl` unusable. '
- :key: '24'
  :contents: "### TPAexec and consensus"
  :translated_contents: ''
  :original_contents: "### TPAexec and consensus "
- :key: '25'
  :contents: These considerations are integrated into TPAexec as well. When deploying
    a cluster using etcd, it constructs a separate DCS cluster per location to facilitate
    high availability in favor of strict consistency.
  :translated_contents: ''
  :original_contents: 'These considerations are integrated into TPAexec as well. When
    deploying a cluster using etcd, it constructs a separate DCS cluster per location
    to facilitate high availability in favor of strict consistency. '
- :key: '26'
  :contents: 'Thus this configuration example groups any DCS nodes assigned to the
    `first` location together, and the  `second` location is a separate cluster:'
  :translated_contents: ''
  :original_contents: 'Thus this configuration example groups any DCS nodes assigned
    to the `first` location together, and the  `second` location is a separate cluster: '
- :key: '27'
  :contents: edb_notranlate_2
  :translated_contents: ''
  :original_contents: " edb_notranlate_2  "
- :key: '28'
  :contents: To override this behavior,  configure the `harp_location` implicitly
    to force a particular grouping.
  :translated_contents: ''
  :original_contents: 'To override this behavior,  configure the `harp_location` implicitly
    to force a particular grouping. '
- :key: '29'
  :contents: 'Thus this example returns all etcd nodes into a single cohesive DCS
    layer:'
  :translated_contents: ''
  :original_contents: 'Thus this example returns all etcd nodes into a single cohesive
    DCS layer: '
- :key: '30'
  :contents: edb_notranlate_3
  :translated_contents: ''
  :original_contents: " edb_notranlate_3  "
- :key: '31'
  :contents: The `harp_location` override might also be necessary to favor specific
    node  groupings when using cloud providers such as Amazon that favor availability
    zones in regions over traditional data centers.
  :translated_contents: ''
  :original_contents: 'The `harp_location` override might also be necessary to favor
    specific node  groupings when using cloud providers such as Amazon that favor
    availability zones in regions over traditional data centers. '
